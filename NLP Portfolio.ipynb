{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e25b4b05-e787-4cff-8fec-5bef89a2a92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/paschalinaparaschou/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/paschalinaparaschou/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/paschalinaparaschou/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/paschalinaparaschou/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import math\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "# download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# URLs of the two documents being compared\n",
    "url_doc1 = 'https://www.ifc.org/en/insights-reports/2023/building-green-in-emerging-markets'\n",
    "url_doc2 = 'https://www.ifc.org/en/insights-reports/2024/emerging-market-green-bonds-2023'\n",
    "\n",
    "# fetch the text from the given URLs\n",
    "def get_text_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    return response.text\n",
    "\n",
    "text_doc1 = get_text_from_url(url_doc1)\n",
    "text_doc2 = get_text_from_url(url_doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59afb280-368e-418b-93e8-d9104fc5aa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: TF-IDF + Cosine Similarity\n",
    "def preprocess_text_enhanced(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    #remove digits and non-word characters\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\d', ' ', text)\n",
    "\n",
    "    # split the text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # set of English stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # remove stop words from the list of words\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # initialize lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # lemmatize each word in the list of words\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# preprocess the text of the two documents\n",
    "preprocessed_text_doc1 = preprocess_text_enhanced(text_doc1)\n",
    "preprocessed_text_doc2 = preprocess_text_enhanced(text_doc2)\n",
    "\n",
    "# initialize the TF-IDF vectorizer\n",
    "vectorizer3 = TfidfVectorizer()\n",
    "\n",
    "# fit the vectorizer on the preprocessed documents and transform them into TF-IDF vectors\n",
    "tfidf_matrix3 = vectorizer3.fit_transform([preprocessed_text_doc1, preprocessed_text_doc2])\n",
    "\n",
    "# cosine similarity\n",
    "cosine_sim_tf = cosine_similarity(tfidf_matrix3[0:1], tfidf_matrix3[1:2])[0][0]\n",
    "#print(cosine_sim_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecdeff09-15e3-46be-b401-548d9aa8698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Jaccard Similarity\n",
    "def preprocess_text_simple(text):\n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # set of English stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def jaccard_similarity(text1, text2):\n",
    "    # preprocess the text\n",
    "    set1 = set(preprocess_text_simple(text1))\n",
    "    set2 = set(preprocess_text_simple(text2))\n",
    "\n",
    "    # calculate the common tokens of the two sets\n",
    "    intersection = set1.intersection(set2)\n",
    "\n",
    "    # calculate unique tokens from both texts\n",
    "    union = set1.union(set2)\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "# cosine similarity\n",
    "jaccard_sim = jaccard_similarity(text_doc1, text_doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0b7f2aa-799a-4960-b1cd-2b9372cd20d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words + Cosine Similarity (Radians): 0.1789\n",
      "File 1:\n",
      "13259 words,\n",
      "1301 distinct words\n",
      "File 2:\n",
      "10634 words,\n",
      "1079 distinct words\n"
     ]
    }
   ],
   "source": [
    "# Method 3: Cosine Similarity with Bag of Words\n",
    "\n",
    "# remove punctuation and convert to lowercase\n",
    "translation_table = str.maketrans(string.punctuation + string.ascii_uppercase, \" \" * len(string.punctuation) + string.ascii_lowercase)\n",
    "\n",
    "def get_words_from_text(text):\n",
    "    text = text.translate(translation_table)\n",
    "\n",
    "    # split the translated text into a list of words\n",
    "    word_list = text.split()\n",
    "    return word_list\n",
    "\n",
    "def count_frequency(word_list):\n",
    "    # initiate empty dictionary\n",
    "    D = {}\n",
    "    for new_word in word_list:\n",
    "        if new_word in D:\n",
    "            D[new_word] += 1\n",
    "        else:\n",
    "            D[new_word] = 1\n",
    "    return D\n",
    "\n",
    "def word_frequencies_for_text(text):\n",
    "    word_list = get_words_from_text(text)\n",
    "    freq_mapping = count_frequency(word_list)\n",
    "    return freq_mapping, len(word_list), len(freq_mapping)\n",
    "\n",
    "def dot_product(D1, D2):\n",
    "    Sum = 0.0\n",
    "    for key in D1:\n",
    "        if key in D2:\n",
    "            Sum += D1[key] * D2[key]\n",
    "    return Sum\n",
    "\n",
    "def vector_angle(D1, D2):\n",
    "    numerator = dot_product(D1, D2)\n",
    "\n",
    "    # calculate the magnitudes of the frequency vectors\n",
    "    denominator = math.sqrt(dot_product(D1, D1) * dot_product(D2, D2))\n",
    "\n",
    "    # return the angle between the two vectors in radians\n",
    "    return math.acos(numerator / denominator)\n",
    "\n",
    "# Word frequency mappings and word counts for the two documents\n",
    "\n",
    "freq_mapping1, words1, distinct_words1 = word_frequencies_for_text(text_doc1)\n",
    "freq_mapping2, words2, distinct_words2 = word_frequencies_for_text(text_doc2)\n",
    "\n",
    "distance = vector_angle(freq_mapping1, freq_mapping2)\n",
    "\n",
    "# results\n",
    "print(f\"Bag of Words + Cosine Similarity (Radians): {distance:.4f}\")\n",
    "\n",
    "print(\"File 1:\")\n",
    "print(f\"{words1} words,\")\n",
    "print(f\"{distinct_words1} distinct words\")\n",
    "\n",
    "print(\"File 2:\")\n",
    "print(f\"{words2} words,\")\n",
    "print(f\"{distinct_words2} distinct words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb7c4ba0-5108-443d-bd9c-83ea5b399615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 4: Doc2Vec + Cosine Similarity\n",
    "def preprocess_text_for_doc2vec(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# preprocess the texts and create TaggedDocument objects\n",
    "documents_for_doc2vec = [TaggedDocument(words=preprocess_text_for_doc2vec(text_doc1), tags=['doc1']),\n",
    "                         TaggedDocument(words=preprocess_text_for_doc2vec(text_doc2), tags=['doc2'])]\n",
    "\n",
    "# initialize the Doc2Vec model\n",
    "model = Doc2Vec(vector_size=100, min_count=2, epochs=40)\n",
    "\n",
    "# build the vocabulary from the preprocessed documents\n",
    "model.build_vocab(documents_for_doc2vec)\n",
    "\n",
    "# train the model\n",
    "model.train(documents_for_doc2vec, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "vector1 = model.infer_vector(preprocess_text_for_doc2vec(text_doc1))\n",
    "vector2 = model.infer_vector(preprocess_text_for_doc2vec(text_doc2))\n",
    "\n",
    "# cosine similarity\n",
    "cosine_sim_doc2vec = cosine_similarity([vector1], [vector2])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a6728aa-7554-49a4-b82c-ca9c81b22005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+--------+\n",
      "|                      Method                      | Result |\n",
      "+--------------------------------------------------+--------+\n",
      "| Enhanced Preprocessed TF-IDF + Cosine Similarity | 0.9802 |\n",
      "|                Jaccard Similarity                | 0.4956 |\n",
      "|    Bag of Words + Cosine Similarity (Radians)    | 0.1789 |\n",
      "|           Doc2Vec + Cosine Similarity            | 0.7845 |\n",
      "+--------------------------------------------------+--------+\n",
      "The best method for similarity check is: 0.9802\n",
      "Please look at the provided word document for further explanation\n"
     ]
    }
   ],
   "source": [
    "# Summary of results table\n",
    "from prettytable import PrettyTable \n",
    "\n",
    "# specify the column names of the table\n",
    "myTable = PrettyTable([\"Method\", \"Result\"]) \n",
    "\n",
    "# add rows to the table\n",
    "myTable.add_row([\"Enhanced Preprocessed TF-IDF + Cosine Similarity\", f\"{cosine_sim_tf:.4f}\" ]) \n",
    "myTable.add_row([\"Jaccard Similarity\", f\"{jaccard_sim:.4f}\" ]) \n",
    "myTable.add_row([\"Bag of Words + Cosine Similarity (Radians)\", f\"{distance:.4f}\" ]) \n",
    "myTable.add_row([\"Doc2Vec + Cosine Similarity\", f\"{cosine_sim_doc2vec:.4f}\"]) \n",
    "print(myTable)\n",
    "\n",
    "print(\"The best method for similarity check is:\", f\"{cosine_sim_tf:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
